{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import args\n",
    "\n",
    "class LoadData():\n",
    "    \"\"\"Load graph data\"\"\"\n",
    "    def __init__(self, file_path, family, samples):\n",
    "        self.family = family\n",
    "        self.samples = samples\n",
    "        self.file_path = file_path\n",
    "        self.nSample = len(samples)\n",
    "\n",
    "    def get_adj_wgh(self):\n",
    "        \"\"\"generating adjacency matrix\"\"\"\n",
    "        adj_orig_list =[]\n",
    "        for sample in self.samples:\n",
    "            f_name = self.file_path + \".\".join([sample])\n",
    "            adj   = np.asarray(pd.read_csv(f_name, index_col = 0, iterator = False))\n",
    "            adj_orig_list.append(adj)    \n",
    "\n",
    "        return adj_orig_list\n",
    "\n",
    "    def get_adj_label(self):\n",
    "        adj_m = self.get_adj_m()\n",
    "        adj_label_list =[]\n",
    "        \n",
    "        for _, adj in enumerate(adj_m):\n",
    "            adj_label = adj + sp.eye(adj.shape[0])\n",
    "            adj_label = sparse_to_tuple(sp.coo_matrix(adj_label))\n",
    "            #adj_label = sparse_to_tuple(adj_label)\n",
    "            adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "            adj_label_list.append(adj_label)\n",
    "        return adj_label_list\n",
    "\n",
    "    def get_adj_norm(self):\n",
    "        adj_m = self.get_adj_m()\n",
    "        adj_norm_list =[]\n",
    "\n",
    "        for _, adj in enumerate(adj_m):\n",
    "            adj_norm = preprocess_graph(adj)\n",
    "            adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "            adj_norm_list.append(adj_norm)\n",
    "\n",
    "        return adj_norm_list\n",
    "\n",
    "    def get_adj_m(self):\n",
    "        adj_wgh = self.get_adj_wgh()\n",
    "        adj_m_list =[]\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            adj[adj>0] = 1\n",
    "            for i in range(38):\n",
    "                adj[i, i] = 0\n",
    "            for i in range(38):\n",
    "                for j in range(38):\n",
    "                    if adj[i, j] == 1:\n",
    "                        adj[j, i] = 1\n",
    "            adj_m_list.append(adj)    \n",
    "        return adj_m_list\n",
    "\n",
    "    def get_feature(self):\n",
    "        \"\"\"generating feature matrix X\"\"\"\n",
    "        adj_wgh = self.get_adj_wgh()\n",
    "        x_list = []\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            x_feature  = adj\n",
    "            x_feature  = csr_matrix(x_feature)\n",
    "            x_feature  = sparse_to_tuple(x_feature)\n",
    "            x_feature  = torch.sparse.FloatTensor(torch.LongTensor(x_feature[0].T), \n",
    "                            torch.FloatTensor(x_feature[1]), \n",
    "                            torch.Size(x_feature[2]))\n",
    "            x_list.append(x_feature)\n",
    "        \n",
    "        return x_list\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    \n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "from os import walk\n",
    "\n",
    "def get_filename(mypath):\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    return f\n",
    "\n",
    "class VGAE(nn.Module):\n",
    "\tdef __init__(self, input_dim, hidden1_dim, hidden2_dim):\n",
    "\t\tsuper(VGAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(input_dim, hidden1_dim)\n",
    "\t\tself.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "\t\tself.gcn_logstddev = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, x, adj):\n",
    "\t\thidden = self.base_gcn(x, adj)\n",
    "\t\tself.mean = self.gcn_mean(hidden, adj)\n",
    "\t\tself.logstd = self.gcn_logstddev(hidden, adj)\n",
    "\t\tgaussian_noise = torch.randn(x.size(0), args.hidden2_dim)\n",
    "\t\tsampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "\t\treturn sampled_z\n",
    "\n",
    "\tdef forward(self, x, adj):\n",
    "\t\tZ = self.encode(x, adj)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, activation = F.relu, **kwargs):\n",
    "\t\tsuper(GraphConvSparse, self).__init__(**kwargs)\n",
    "\t\tself.weight = glorot_init(input_dim, output_dim) \n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, inputs, adj):\n",
    "\t\tx = inputs\n",
    "        \n",
    "\t\tx = torch.mm(x,self.weight)\n",
    "\t\tx = torch.mm(adj, x)\n",
    "\t\toutputs = self.activation(x)\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "\tA_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "\treturn A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "\tinit_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "\tinitial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "\treturn nn.Parameter(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(15.1088, grad_fn=<MulBackward0>) 0\n",
      "Loss: tensor(13.1328, grad_fn=<MulBackward0>) 20\n",
      "Loss: tensor(11.9995, grad_fn=<MulBackward0>) 40\n",
      "Loss: tensor(10.3099, grad_fn=<MulBackward0>) 60\n",
      "Loss: tensor(9.3911, grad_fn=<MulBackward0>) 80\n",
      "Loss: tensor(8.3010, grad_fn=<MulBackward0>) 100\n",
      "Loss: tensor(7.6526, grad_fn=<MulBackward0>) 120\n",
      "Loss: tensor(7.0333, grad_fn=<MulBackward0>) 140\n",
      "Loss: tensor(7.1112, grad_fn=<MulBackward0>) 160\n",
      "Loss: tensor(7.2037, grad_fn=<MulBackward0>) 180\n",
      "Loss: tensor(7.7984, grad_fn=<MulBackward0>) 200\n",
      "Loss: tensor(7.5850, grad_fn=<MulBackward0>) 220\n",
      "Loss: tensor(8.5920, grad_fn=<MulBackward0>) 240\n",
      "Loss: tensor(8.6071, grad_fn=<MulBackward0>) 260\n",
      "Loss: tensor(9.3125, grad_fn=<MulBackward0>) 280\n",
      "Loss: tensor(10.3056, grad_fn=<MulBackward0>) 300\n",
      "Loss: tensor(10.1636, grad_fn=<MulBackward0>) 320\n",
      "Loss: tensor(10.7128, grad_fn=<MulBackward0>) 340\n",
      "Loss: tensor(10.6039, grad_fn=<MulBackward0>) 360\n",
      "Loss: tensor(9.9830, grad_fn=<MulBackward0>) 380\n",
      "Loss: tensor(9.9892, grad_fn=<MulBackward0>) 400\n",
      "Loss: tensor(9.8411, grad_fn=<MulBackward0>) 420\n",
      "Loss: tensor(10.0319, grad_fn=<MulBackward0>) 440\n",
      "Loss: tensor(9.8929, grad_fn=<MulBackward0>) 460\n",
      "Loss: tensor(9.4092, grad_fn=<MulBackward0>) 480\n",
      "Loss: tensor(8.4803, grad_fn=<MulBackward0>) 500\n",
      "Loss: tensor(8.2570, grad_fn=<MulBackward0>) 520\n",
      "Loss: tensor(7.9425, grad_fn=<MulBackward0>) 540\n",
      "Loss: tensor(7.3082, grad_fn=<MulBackward0>) 560\n",
      "Loss: tensor(7.5059, grad_fn=<MulBackward0>) 580\n",
      "Loss: tensor(6.8130, grad_fn=<MulBackward0>) 600\n",
      "Loss: tensor(6.8216, grad_fn=<MulBackward0>) 620\n",
      "Loss: tensor(6.4471, grad_fn=<MulBackward0>) 640\n",
      "Loss: tensor(6.4380, grad_fn=<MulBackward0>) 660\n",
      "Loss: tensor(6.1952, grad_fn=<MulBackward0>) 680\n",
      "Loss: tensor(5.9983, grad_fn=<MulBackward0>) 700\n",
      "Loss: tensor(6.2366, grad_fn=<MulBackward0>) 720\n",
      "Loss: tensor(5.8504, grad_fn=<MulBackward0>) 740\n",
      "Loss: tensor(5.7772, grad_fn=<MulBackward0>) 760\n",
      "Loss: tensor(5.9881, grad_fn=<MulBackward0>) 780\n",
      "Loss: tensor(5.6618, grad_fn=<MulBackward0>) 800\n",
      "Loss: tensor(5.5156, grad_fn=<MulBackward0>) 820\n",
      "Loss: tensor(5.4735, grad_fn=<MulBackward0>) 840\n",
      "Loss: tensor(5.4357, grad_fn=<MulBackward0>) 860\n",
      "Loss: tensor(5.5857, grad_fn=<MulBackward0>) 880\n",
      "Loss: tensor(5.4802, grad_fn=<MulBackward0>) 900\n",
      "Loss: tensor(5.4217, grad_fn=<MulBackward0>) 920\n",
      "Loss: tensor(5.3448, grad_fn=<MulBackward0>) 940\n",
      "Loss: tensor(5.6420, grad_fn=<MulBackward0>) 960\n",
      "Loss: tensor(5.6476, grad_fn=<MulBackward0>) 980\n",
      "Loss: tensor(5.3958, grad_fn=<MulBackward0>) 1000\n",
      "Loss: tensor(5.5520, grad_fn=<MulBackward0>) 1020\n",
      "Loss: tensor(5.2399, grad_fn=<MulBackward0>) 1040\n",
      "Loss: tensor(5.2935, grad_fn=<MulBackward0>) 1060\n",
      "Loss: tensor(5.3881, grad_fn=<MulBackward0>) 1080\n",
      "Loss: tensor(5.4299, grad_fn=<MulBackward0>) 1100\n",
      "Loss: tensor(5.3033, grad_fn=<MulBackward0>) 1120\n",
      "Loss: tensor(5.1190, grad_fn=<MulBackward0>) 1140\n",
      "Loss: tensor(5.4782, grad_fn=<MulBackward0>) 1160\n",
      "Loss: tensor(5.2985, grad_fn=<MulBackward0>) 1180\n",
      "Loss: tensor(5.2010, grad_fn=<MulBackward0>) 1200\n",
      "Loss: tensor(5.3108, grad_fn=<MulBackward0>) 1220\n",
      "Loss: tensor(5.3017, grad_fn=<MulBackward0>) 1240\n",
      "Loss: tensor(4.9083, grad_fn=<MulBackward0>) 1260\n",
      "Loss: tensor(5.0170, grad_fn=<MulBackward0>) 1280\n",
      "Loss: tensor(5.3738, grad_fn=<MulBackward0>) 1300\n",
      "Loss: tensor(4.8998, grad_fn=<MulBackward0>) 1320\n",
      "Loss: tensor(4.8575, grad_fn=<MulBackward0>) 1340\n",
      "Loss: tensor(5.0157, grad_fn=<MulBackward0>) 1360\n",
      "Loss: tensor(4.9456, grad_fn=<MulBackward0>) 1380\n",
      "Loss: tensor(5.0426, grad_fn=<MulBackward0>) 1400\n",
      "Loss: tensor(4.6980, grad_fn=<MulBackward0>) 1420\n",
      "Loss: tensor(4.8933, grad_fn=<MulBackward0>) 1440\n",
      "Loss: tensor(4.8935, grad_fn=<MulBackward0>) 1460\n",
      "Loss: tensor(4.7786, grad_fn=<MulBackward0>) 1480\n",
      "Loss: tensor(4.8196, grad_fn=<MulBackward0>) 1500\n",
      "Loss: tensor(4.7876, grad_fn=<MulBackward0>) 1520\n",
      "Loss: tensor(4.7927, grad_fn=<MulBackward0>) 1540\n",
      "Loss: tensor(4.5628, grad_fn=<MulBackward0>) 1560\n",
      "Loss: tensor(4.5764, grad_fn=<MulBackward0>) 1580\n",
      "Loss: tensor(4.7842, grad_fn=<MulBackward0>) 1600\n",
      "Loss: tensor(4.2649, grad_fn=<MulBackward0>) 1620\n",
      "Loss: tensor(4.6523, grad_fn=<MulBackward0>) 1640\n",
      "Loss: tensor(4.5132, grad_fn=<MulBackward0>) 1660\n",
      "Loss: tensor(4.3649, grad_fn=<MulBackward0>) 1680\n",
      "Loss: tensor(4.3026, grad_fn=<MulBackward0>) 1700\n",
      "Loss: tensor(4.4375, grad_fn=<MulBackward0>) 1720\n",
      "Loss: tensor(4.3803, grad_fn=<MulBackward0>) 1740\n",
      "Loss: tensor(4.2620, grad_fn=<MulBackward0>) 1760\n",
      "Loss: tensor(4.5547, grad_fn=<MulBackward0>) 1780\n",
      "Loss: tensor(4.2922, grad_fn=<MulBackward0>) 1800\n",
      "Loss: tensor(4.4134, grad_fn=<MulBackward0>) 1820\n",
      "Loss: tensor(4.4039, grad_fn=<MulBackward0>) 1840\n",
      "Loss: tensor(4.4405, grad_fn=<MulBackward0>) 1860\n",
      "Loss: tensor(4.3878, grad_fn=<MulBackward0>) 1880\n",
      "Loss: tensor(4.0422, grad_fn=<MulBackward0>) 1900\n",
      "Loss: tensor(4.3756, grad_fn=<MulBackward0>) 1920\n",
      "Loss: tensor(4.3362, grad_fn=<MulBackward0>) 1940\n",
      "Loss: tensor(4.1368, grad_fn=<MulBackward0>) 1960\n",
      "Loss: tensor(4.1564, grad_fn=<MulBackward0>) 1980\n",
      "Loss: tensor(4.1076, grad_fn=<MulBackward0>) 2000\n",
      "Loss: tensor(4.1005, grad_fn=<MulBackward0>) 2020\n",
      "Loss: tensor(4.4403, grad_fn=<MulBackward0>) 2040\n",
      "Loss: tensor(4.0794, grad_fn=<MulBackward0>) 2060\n",
      "Loss: tensor(4.1789, grad_fn=<MulBackward0>) 2080\n",
      "Loss: tensor(4.1160, grad_fn=<MulBackward0>) 2100\n",
      "Loss: tensor(4.0830, grad_fn=<MulBackward0>) 2120\n",
      "Loss: tensor(4.1763, grad_fn=<MulBackward0>) 2140\n",
      "Loss: tensor(3.9832, grad_fn=<MulBackward0>) 2160\n",
      "Loss: tensor(4.0705, grad_fn=<MulBackward0>) 2180\n",
      "Loss: tensor(4.2431, grad_fn=<MulBackward0>) 2200\n",
      "Loss: tensor(4.2541, grad_fn=<MulBackward0>) 2220\n",
      "Loss: tensor(3.9617, grad_fn=<MulBackward0>) 2240\n",
      "Loss: tensor(3.7059, grad_fn=<MulBackward0>) 2260\n",
      "Loss: tensor(3.8866, grad_fn=<MulBackward0>) 2280\n",
      "Loss: tensor(3.7860, grad_fn=<MulBackward0>) 2300\n",
      "Loss: tensor(3.9631, grad_fn=<MulBackward0>) 2320\n",
      "Loss: tensor(3.8112, grad_fn=<MulBackward0>) 2340\n",
      "Loss: tensor(3.8000, grad_fn=<MulBackward0>) 2360\n",
      "Loss: tensor(3.8312, grad_fn=<MulBackward0>) 2380\n",
      "Loss: tensor(3.7687, grad_fn=<MulBackward0>) 2400\n",
      "Loss: tensor(3.8409, grad_fn=<MulBackward0>) 2420\n",
      "Loss: tensor(3.9918, grad_fn=<MulBackward0>) 2440\n",
      "Loss: tensor(3.7793, grad_fn=<MulBackward0>) 2460\n",
      "Loss: tensor(3.7752, grad_fn=<MulBackward0>) 2480\n",
      "Loss: tensor(3.8420, grad_fn=<MulBackward0>) 2500\n",
      "Loss: tensor(3.8365, grad_fn=<MulBackward0>) 2520\n",
      "Loss: tensor(3.8850, grad_fn=<MulBackward0>) 2540\n",
      "Loss: tensor(3.7231, grad_fn=<MulBackward0>) 2560\n",
      "Loss: tensor(3.8482, grad_fn=<MulBackward0>) 2580\n",
      "Loss: tensor(3.9091, grad_fn=<MulBackward0>) 2600\n",
      "Loss: tensor(3.7827, grad_fn=<MulBackward0>) 2620\n",
      "Loss: tensor(3.7257, grad_fn=<MulBackward0>) 2640\n",
      "Loss: tensor(3.6944, grad_fn=<MulBackward0>) 2660\n",
      "Loss: tensor(3.6866, grad_fn=<MulBackward0>) 2680\n",
      "Loss: tensor(3.7741, grad_fn=<MulBackward0>) 2700\n",
      "Loss: tensor(3.6544, grad_fn=<MulBackward0>) 2720\n",
      "Loss: tensor(3.5995, grad_fn=<MulBackward0>) 2740\n",
      "Loss: tensor(3.6335, grad_fn=<MulBackward0>) 2760\n",
      "Loss: tensor(3.8072, grad_fn=<MulBackward0>) 2780\n",
      "Loss: tensor(3.6161, grad_fn=<MulBackward0>) 2800\n",
      "Loss: tensor(3.6278, grad_fn=<MulBackward0>) 2820\n",
      "Loss: tensor(3.6846, grad_fn=<MulBackward0>) 2840\n",
      "Loss: tensor(3.5659, grad_fn=<MulBackward0>) 2860\n",
      "Loss: tensor(3.6514, grad_fn=<MulBackward0>) 2880\n",
      "Loss: tensor(3.6505, grad_fn=<MulBackward0>) 2900\n",
      "Loss: tensor(3.5675, grad_fn=<MulBackward0>) 2920\n",
      "Loss: tensor(3.6481, grad_fn=<MulBackward0>) 2940\n",
      "Loss: tensor(3.6771, grad_fn=<MulBackward0>) 2960\n",
      "Loss: tensor(3.5878, grad_fn=<MulBackward0>) 2980\n",
      "Loss: tensor(3.7221, grad_fn=<MulBackward0>) 3000\n",
      "Loss: tensor(3.5276, grad_fn=<MulBackward0>) 3020\n",
      "Loss: tensor(3.8521, grad_fn=<MulBackward0>) 3040\n",
      "Loss: tensor(3.5077, grad_fn=<MulBackward0>) 3060\n",
      "Loss: tensor(3.6947, grad_fn=<MulBackward0>) 3080\n",
      "Loss: tensor(3.5273, grad_fn=<MulBackward0>) 3100\n",
      "Loss: tensor(3.7068, grad_fn=<MulBackward0>) 3120\n",
      "Loss: tensor(3.6919, grad_fn=<MulBackward0>) 3140\n",
      "Loss: tensor(3.5481, grad_fn=<MulBackward0>) 3160\n",
      "Loss: tensor(3.5663, grad_fn=<MulBackward0>) 3180\n",
      "Loss: tensor(3.5816, grad_fn=<MulBackward0>) 3200\n",
      "Loss: tensor(3.5117, grad_fn=<MulBackward0>) 3220\n",
      "Loss: tensor(3.5818, grad_fn=<MulBackward0>) 3240\n",
      "Loss: tensor(3.5677, grad_fn=<MulBackward0>) 3260\n",
      "Loss: tensor(3.5408, grad_fn=<MulBackward0>) 3280\n",
      "Loss: tensor(3.5654, grad_fn=<MulBackward0>) 3300\n",
      "Loss: tensor(3.5806, grad_fn=<MulBackward0>) 3320\n",
      "Loss: tensor(3.5207, grad_fn=<MulBackward0>) 3340\n",
      "Loss: tensor(3.6468, grad_fn=<MulBackward0>) 3360\n",
      "Loss: tensor(3.5485, grad_fn=<MulBackward0>) 3380\n",
      "Loss: tensor(3.5080, grad_fn=<MulBackward0>) 3400\n",
      "Loss: tensor(3.4116, grad_fn=<MulBackward0>) 3420\n",
      "Loss: tensor(3.6301, grad_fn=<MulBackward0>) 3440\n",
      "Loss: tensor(3.3895, grad_fn=<MulBackward0>) 3460\n",
      "Loss: tensor(3.6143, grad_fn=<MulBackward0>) 3480\n",
      "Loss: tensor(3.6046, grad_fn=<MulBackward0>) 3500\n",
      "Loss: tensor(3.4282, grad_fn=<MulBackward0>) 3520\n",
      "Loss: tensor(3.4726, grad_fn=<MulBackward0>) 3540\n",
      "Loss: tensor(3.5939, grad_fn=<MulBackward0>) 3560\n",
      "Loss: tensor(3.6273, grad_fn=<MulBackward0>) 3580\n",
      "Loss: tensor(3.5973, grad_fn=<MulBackward0>) 3600\n",
      "Loss: tensor(3.6733, grad_fn=<MulBackward0>) 3620\n",
      "Loss: tensor(3.6209, grad_fn=<MulBackward0>) 3640\n",
      "Loss: tensor(3.4409, grad_fn=<MulBackward0>) 3660\n",
      "Loss: tensor(3.5280, grad_fn=<MulBackward0>) 3680\n",
      "Loss: tensor(3.5172, grad_fn=<MulBackward0>) 3700\n",
      "Loss: tensor(3.5042, grad_fn=<MulBackward0>) 3720\n",
      "Loss: tensor(3.4745, grad_fn=<MulBackward0>) 3740\n",
      "Loss: tensor(3.4788, grad_fn=<MulBackward0>) 3760\n",
      "Loss: tensor(3.4232, grad_fn=<MulBackward0>) 3780\n",
      "Loss: tensor(3.4649, grad_fn=<MulBackward0>) 3800\n",
      "Loss: tensor(3.4171, grad_fn=<MulBackward0>) 3820\n",
      "Loss: tensor(3.4548, grad_fn=<MulBackward0>) 3840\n",
      "Loss: tensor(3.5415, grad_fn=<MulBackward0>) 3860\n",
      "Loss: tensor(3.4923, grad_fn=<MulBackward0>) 3880\n",
      "Loss: tensor(3.5715, grad_fn=<MulBackward0>) 3900\n",
      "Loss: tensor(3.5146, grad_fn=<MulBackward0>) 3920\n",
      "Loss: tensor(3.5620, grad_fn=<MulBackward0>) 3940\n",
      "Loss: tensor(3.4881, grad_fn=<MulBackward0>) 3960\n",
      "Loss: tensor(3.4703, grad_fn=<MulBackward0>) 3980\n",
      "Loss: tensor(3.5224, grad_fn=<MulBackward0>) 4000\n",
      "Loss: tensor(3.4853, grad_fn=<MulBackward0>) 4020\n",
      "Loss: tensor(3.4161, grad_fn=<MulBackward0>) 4040\n",
      "Loss: tensor(3.4861, grad_fn=<MulBackward0>) 4060\n",
      "Loss: tensor(3.4594, grad_fn=<MulBackward0>) 4080\n",
      "Loss: tensor(3.4546, grad_fn=<MulBackward0>) 4100\n",
      "Loss: tensor(3.3579, grad_fn=<MulBackward0>) 4120\n",
      "Loss: tensor(3.4861, grad_fn=<MulBackward0>) 4140\n",
      "Loss: tensor(3.4926, grad_fn=<MulBackward0>) 4160\n",
      "Loss: tensor(3.4968, grad_fn=<MulBackward0>) 4180\n",
      "Loss: tensor(3.3942, grad_fn=<MulBackward0>) 4200\n",
      "Loss: tensor(3.4719, grad_fn=<MulBackward0>) 4220\n",
      "Loss: tensor(3.5279, grad_fn=<MulBackward0>) 4240\n",
      "Loss: tensor(3.4386, grad_fn=<MulBackward0>) 4260\n",
      "Loss: tensor(3.4426, grad_fn=<MulBackward0>) 4280\n",
      "Loss: tensor(3.4524, grad_fn=<MulBackward0>) 4300\n",
      "Loss: tensor(3.5346, grad_fn=<MulBackward0>) 4320\n",
      "Loss: tensor(3.3801, grad_fn=<MulBackward0>) 4340\n",
      "Loss: tensor(3.4037, grad_fn=<MulBackward0>) 4360\n",
      "Loss: tensor(3.4977, grad_fn=<MulBackward0>) 4380\n",
      "Loss: tensor(3.4112, grad_fn=<MulBackward0>) 4400\n",
      "Loss: tensor(3.3524, grad_fn=<MulBackward0>) 4420\n",
      "Loss: tensor(3.4452, grad_fn=<MulBackward0>) 4440\n",
      "Loss: tensor(3.4955, grad_fn=<MulBackward0>) 4460\n",
      "Loss: tensor(3.5487, grad_fn=<MulBackward0>) 4480\n",
      "Loss: tensor(3.5444, grad_fn=<MulBackward0>) 4500\n",
      "Loss: tensor(3.3653, grad_fn=<MulBackward0>) 4520\n",
      "Loss: tensor(3.4154, grad_fn=<MulBackward0>) 4540\n",
      "Loss: tensor(3.3371, grad_fn=<MulBackward0>) 4560\n",
      "Loss: tensor(3.4270, grad_fn=<MulBackward0>) 4580\n",
      "Loss: tensor(3.3902, grad_fn=<MulBackward0>) 4600\n",
      "Loss: tensor(3.3580, grad_fn=<MulBackward0>) 4620\n",
      "Loss: tensor(3.4408, grad_fn=<MulBackward0>) 4640\n",
      "Loss: tensor(3.4519, grad_fn=<MulBackward0>) 4660\n",
      "Loss: tensor(3.4309, grad_fn=<MulBackward0>) 4680\n",
      "Loss: tensor(3.4672, grad_fn=<MulBackward0>) 4700\n",
      "Loss: tensor(3.3815, grad_fn=<MulBackward0>) 4720\n",
      "Loss: tensor(3.4334, grad_fn=<MulBackward0>) 4740\n",
      "Loss: tensor(3.4062, grad_fn=<MulBackward0>) 4760\n",
      "Loss: tensor(3.4635, grad_fn=<MulBackward0>) 4780\n",
      "Loss: tensor(3.3726, grad_fn=<MulBackward0>) 4800\n",
      "Loss: tensor(3.5659, grad_fn=<MulBackward0>) 4820\n",
      "Loss: tensor(3.4631, grad_fn=<MulBackward0>) 4840\n",
      "Loss: tensor(3.5004, grad_fn=<MulBackward0>) 4860\n",
      "Loss: tensor(3.4089, grad_fn=<MulBackward0>) 4880\n",
      "Loss: tensor(3.4641, grad_fn=<MulBackward0>) 4900\n",
      "Loss: tensor(3.3525, grad_fn=<MulBackward0>) 4920\n",
      "Loss: tensor(3.4192, grad_fn=<MulBackward0>) 4940\n",
      "Loss: tensor(3.5092, grad_fn=<MulBackward0>) 4960\n",
      "Loss: tensor(3.3665, grad_fn=<MulBackward0>) 4980\n",
      "Loss: tensor(3.4577, grad_fn=<MulBackward0>) 5000\n",
      "Loss: tensor(3.3690, grad_fn=<MulBackward0>) 5020\n",
      "Loss: tensor(3.5095, grad_fn=<MulBackward0>) 5040\n",
      "Loss: tensor(3.3593, grad_fn=<MulBackward0>) 5060\n",
      "Loss: tensor(3.5848, grad_fn=<MulBackward0>) 5080\n",
      "Loss: tensor(3.5039, grad_fn=<MulBackward0>) 5100\n",
      "Loss: tensor(3.4617, grad_fn=<MulBackward0>) 5120\n",
      "Loss: tensor(3.4197, grad_fn=<MulBackward0>) 5140\n",
      "Loss: tensor(3.4823, grad_fn=<MulBackward0>) 5160\n",
      "Loss: tensor(3.4824, grad_fn=<MulBackward0>) 5180\n",
      "Loss: tensor(3.2901, grad_fn=<MulBackward0>) 5200\n",
      "Loss: tensor(3.4344, grad_fn=<MulBackward0>) 5220\n",
      "Loss: tensor(3.3215, grad_fn=<MulBackward0>) 5240\n",
      "Loss: tensor(3.4134, grad_fn=<MulBackward0>) 5260\n",
      "Loss: tensor(3.4226, grad_fn=<MulBackward0>) 5280\n",
      "Loss: tensor(3.5120, grad_fn=<MulBackward0>) 5300\n",
      "Loss: tensor(3.4151, grad_fn=<MulBackward0>) 5320\n",
      "Loss: tensor(3.4624, grad_fn=<MulBackward0>) 5340\n",
      "Loss: tensor(3.3187, grad_fn=<MulBackward0>) 5360\n",
      "Loss: tensor(3.4020, grad_fn=<MulBackward0>) 5380\n",
      "Loss: tensor(3.4140, grad_fn=<MulBackward0>) 5400\n",
      "Loss: tensor(3.3755, grad_fn=<MulBackward0>) 5420\n",
      "Loss: tensor(3.3866, grad_fn=<MulBackward0>) 5440\n",
      "Loss: tensor(3.5403, grad_fn=<MulBackward0>) 5460\n",
      "Loss: tensor(3.3995, grad_fn=<MulBackward0>) 5480\n",
      "Loss: tensor(3.3656, grad_fn=<MulBackward0>) 5500\n",
      "Loss: tensor(3.4103, grad_fn=<MulBackward0>) 5520\n",
      "Loss: tensor(3.3273, grad_fn=<MulBackward0>) 5540\n",
      "Loss: tensor(3.2886, grad_fn=<MulBackward0>) 5560\n",
      "Loss: tensor(3.3387, grad_fn=<MulBackward0>) 5580\n",
      "Loss: tensor(3.4038, grad_fn=<MulBackward0>) 5600\n",
      "Loss: tensor(3.4560, grad_fn=<MulBackward0>) 5620\n",
      "Loss: tensor(3.3855, grad_fn=<MulBackward0>) 5640\n",
      "Loss: tensor(3.4156, grad_fn=<MulBackward0>) 5660\n",
      "Loss: tensor(3.4323, grad_fn=<MulBackward0>) 5680\n",
      "Loss: tensor(3.4028, grad_fn=<MulBackward0>) 5700\n",
      "Loss: tensor(3.4118, grad_fn=<MulBackward0>) 5720\n",
      "Loss: tensor(3.3878, grad_fn=<MulBackward0>) 5740\n",
      "Loss: tensor(3.4057, grad_fn=<MulBackward0>) 5760\n",
      "Loss: tensor(3.4282, grad_fn=<MulBackward0>) 5780\n",
      "Loss: tensor(3.3573, grad_fn=<MulBackward0>) 5800\n",
      "Loss: tensor(3.3057, grad_fn=<MulBackward0>) 5820\n",
      "Loss: tensor(3.3792, grad_fn=<MulBackward0>) 5840\n",
      "Loss: tensor(3.3776, grad_fn=<MulBackward0>) 5860\n",
      "Loss: tensor(3.3846, grad_fn=<MulBackward0>) 5880\n",
      "Loss: tensor(3.3598, grad_fn=<MulBackward0>) 5900\n",
      "Loss: tensor(3.3837, grad_fn=<MulBackward0>) 5920\n",
      "Loss: tensor(3.3477, grad_fn=<MulBackward0>) 5940\n",
      "Loss: tensor(3.3438, grad_fn=<MulBackward0>) 5960\n",
      "Loss: tensor(3.3242, grad_fn=<MulBackward0>) 5980\n",
      "Loss: tensor(3.4300, grad_fn=<MulBackward0>) 6000\n",
      "Loss: tensor(3.3415, grad_fn=<MulBackward0>) 6020\n",
      "Loss: tensor(3.4748, grad_fn=<MulBackward0>) 6040\n",
      "Loss: tensor(3.3912, grad_fn=<MulBackward0>) 6060\n",
      "Loss: tensor(3.3916, grad_fn=<MulBackward0>) 6080\n",
      "Loss: tensor(3.3725, grad_fn=<MulBackward0>) 6100\n",
      "Loss: tensor(3.3109, grad_fn=<MulBackward0>) 6120\n",
      "Loss: tensor(3.4380, grad_fn=<MulBackward0>) 6140\n",
      "Loss: tensor(3.3512, grad_fn=<MulBackward0>) 6160\n",
      "Loss: tensor(3.3694, grad_fn=<MulBackward0>) 6180\n",
      "Loss: tensor(3.3437, grad_fn=<MulBackward0>) 6200\n",
      "Loss: tensor(3.3119, grad_fn=<MulBackward0>) 6220\n",
      "Loss: tensor(3.4178, grad_fn=<MulBackward0>) 6240\n",
      "Loss: tensor(3.4066, grad_fn=<MulBackward0>) 6260\n",
      "Loss: tensor(3.3213, grad_fn=<MulBackward0>) 6280\n",
      "Loss: tensor(3.4361, grad_fn=<MulBackward0>) 6300\n",
      "Loss: tensor(3.4115, grad_fn=<MulBackward0>) 6320\n",
      "Loss: tensor(3.3566, grad_fn=<MulBackward0>) 6340\n",
      "Loss: tensor(3.4033, grad_fn=<MulBackward0>) 6360\n",
      "Loss: tensor(3.3057, grad_fn=<MulBackward0>) 6380\n",
      "Loss: tensor(3.2795, grad_fn=<MulBackward0>) 6400\n",
      "Loss: tensor(3.3681, grad_fn=<MulBackward0>) 6420\n",
      "Loss: tensor(3.3421, grad_fn=<MulBackward0>) 6440\n",
      "Loss: tensor(3.2454, grad_fn=<MulBackward0>) 6460\n",
      "Loss: tensor(3.4184, grad_fn=<MulBackward0>) 6480\n",
      "Loss: tensor(3.2962, grad_fn=<MulBackward0>) 6500\n",
      "Loss: tensor(3.4316, grad_fn=<MulBackward0>) 6520\n",
      "Loss: tensor(3.3592, grad_fn=<MulBackward0>) 6540\n",
      "Loss: tensor(3.2995, grad_fn=<MulBackward0>) 6560\n",
      "Loss: tensor(3.3853, grad_fn=<MulBackward0>) 6580\n",
      "Loss: tensor(3.3336, grad_fn=<MulBackward0>) 6600\n",
      "Loss: tensor(3.3482, grad_fn=<MulBackward0>) 6620\n",
      "Loss: tensor(3.4299, grad_fn=<MulBackward0>) 6640\n",
      "Loss: tensor(3.4022, grad_fn=<MulBackward0>) 6660\n",
      "Loss: tensor(3.3604, grad_fn=<MulBackward0>) 6680\n",
      "Loss: tensor(3.3619, grad_fn=<MulBackward0>) 6700\n",
      "Loss: tensor(3.3616, grad_fn=<MulBackward0>) 6720\n",
      "Loss: tensor(3.3271, grad_fn=<MulBackward0>) 6740\n",
      "Loss: tensor(3.3195, grad_fn=<MulBackward0>) 6760\n",
      "Loss: tensor(3.3459, grad_fn=<MulBackward0>) 6780\n",
      "Loss: tensor(3.3967, grad_fn=<MulBackward0>) 6800\n",
      "Loss: tensor(3.4337, grad_fn=<MulBackward0>) 6820\n",
      "Loss: tensor(3.4140, grad_fn=<MulBackward0>) 6840\n",
      "Loss: tensor(3.4409, grad_fn=<MulBackward0>) 6860\n",
      "Loss: tensor(3.3034, grad_fn=<MulBackward0>) 6880\n",
      "Loss: tensor(3.4010, grad_fn=<MulBackward0>) 6900\n",
      "Loss: tensor(3.3504, grad_fn=<MulBackward0>) 6920\n",
      "Loss: tensor(3.2943, grad_fn=<MulBackward0>) 6940\n",
      "Loss: tensor(3.2705, grad_fn=<MulBackward0>) 6960\n",
      "Loss: tensor(3.3334, grad_fn=<MulBackward0>) 6980\n",
      "Loss: tensor(3.2984, grad_fn=<MulBackward0>) 7000\n",
      "Loss: tensor(3.3279, grad_fn=<MulBackward0>) 7020\n",
      "Loss: tensor(3.3240, grad_fn=<MulBackward0>) 7040\n",
      "Loss: tensor(3.3996, grad_fn=<MulBackward0>) 7060\n",
      "Loss: tensor(3.3444, grad_fn=<MulBackward0>) 7080\n",
      "Loss: tensor(3.3759, grad_fn=<MulBackward0>) 7100\n",
      "Loss: tensor(3.4570, grad_fn=<MulBackward0>) 7120\n",
      "Loss: tensor(3.3149, grad_fn=<MulBackward0>) 7140\n",
      "Loss: tensor(3.3497, grad_fn=<MulBackward0>) 7160\n",
      "Loss: tensor(3.3984, grad_fn=<MulBackward0>) 7180\n",
      "Loss: tensor(3.3234, grad_fn=<MulBackward0>) 7200\n",
      "Loss: tensor(3.3248, grad_fn=<MulBackward0>) 7220\n",
      "Loss: tensor(3.3418, grad_fn=<MulBackward0>) 7240\n",
      "Loss: tensor(3.3471, grad_fn=<MulBackward0>) 7260\n",
      "Loss: tensor(3.3039, grad_fn=<MulBackward0>) 7280\n",
      "Loss: tensor(3.3372, grad_fn=<MulBackward0>) 7300\n",
      "Loss: tensor(3.3038, grad_fn=<MulBackward0>) 7320\n",
      "Loss: tensor(3.3525, grad_fn=<MulBackward0>) 7340\n",
      "Loss: tensor(3.3032, grad_fn=<MulBackward0>) 7360\n",
      "Loss: tensor(3.2909, grad_fn=<MulBackward0>) 7380\n",
      "Loss: tensor(3.3273, grad_fn=<MulBackward0>) 7400\n",
      "Loss: tensor(3.3076, grad_fn=<MulBackward0>) 7420\n",
      "Loss: tensor(3.3567, grad_fn=<MulBackward0>) 7440\n",
      "Loss: tensor(3.4088, grad_fn=<MulBackward0>) 7460\n",
      "Loss: tensor(3.2862, grad_fn=<MulBackward0>) 7480\n",
      "Loss: tensor(3.3747, grad_fn=<MulBackward0>) 7500\n",
      "Loss: tensor(3.2471, grad_fn=<MulBackward0>) 7520\n",
      "Loss: tensor(3.4229, grad_fn=<MulBackward0>) 7540\n",
      "Loss: tensor(3.4440, grad_fn=<MulBackward0>) 7560\n",
      "Loss: tensor(3.3465, grad_fn=<MulBackward0>) 7580\n",
      "Loss: tensor(3.2742, grad_fn=<MulBackward0>) 7600\n",
      "Loss: tensor(3.3345, grad_fn=<MulBackward0>) 7620\n",
      "Loss: tensor(3.3714, grad_fn=<MulBackward0>) 7640\n",
      "Loss: tensor(3.3831, grad_fn=<MulBackward0>) 7660\n",
      "Loss: tensor(3.2895, grad_fn=<MulBackward0>) 7680\n",
      "Loss: tensor(3.3650, grad_fn=<MulBackward0>) 7700\n",
      "Loss: tensor(3.2858, grad_fn=<MulBackward0>) 7720\n",
      "Loss: tensor(3.3911, grad_fn=<MulBackward0>) 7740\n",
      "Loss: tensor(3.4273, grad_fn=<MulBackward0>) 7760\n",
      "Loss: tensor(3.3212, grad_fn=<MulBackward0>) 7780\n",
      "Loss: tensor(3.3638, grad_fn=<MulBackward0>) 7800\n",
      "Loss: tensor(3.3855, grad_fn=<MulBackward0>) 7820\n",
      "Loss: tensor(3.3457, grad_fn=<MulBackward0>) 7840\n",
      "Loss: tensor(3.3413, grad_fn=<MulBackward0>) 7860\n",
      "Loss: tensor(3.3072, grad_fn=<MulBackward0>) 7880\n",
      "Loss: tensor(3.3095, grad_fn=<MulBackward0>) 7900\n",
      "Loss: tensor(3.3060, grad_fn=<MulBackward0>) 7920\n",
      "Loss: tensor(3.2683, grad_fn=<MulBackward0>) 7940\n",
      "Loss: tensor(3.3356, grad_fn=<MulBackward0>) 7960\n",
      "Loss: tensor(3.4073, grad_fn=<MulBackward0>) 7980\n",
      "Loss: tensor(3.2953, grad_fn=<MulBackward0>) 8000\n",
      "Loss: tensor(3.3711, grad_fn=<MulBackward0>) 8020\n",
      "Loss: tensor(3.3425, grad_fn=<MulBackward0>) 8040\n",
      "Loss: tensor(3.4362, grad_fn=<MulBackward0>) 8060\n",
      "Loss: tensor(3.4268, grad_fn=<MulBackward0>) 8080\n",
      "Loss: tensor(3.3034, grad_fn=<MulBackward0>) 8100\n",
      "Loss: tensor(3.4120, grad_fn=<MulBackward0>) 8120\n",
      "Loss: tensor(3.3142, grad_fn=<MulBackward0>) 8140\n",
      "Loss: tensor(3.3050, grad_fn=<MulBackward0>) 8160\n",
      "Loss: tensor(3.3969, grad_fn=<MulBackward0>) 8180\n",
      "Loss: tensor(3.3595, grad_fn=<MulBackward0>) 8200\n",
      "Loss: tensor(3.4165, grad_fn=<MulBackward0>) 8220\n",
      "Loss: tensor(3.3005, grad_fn=<MulBackward0>) 8240\n",
      "Loss: tensor(3.3920, grad_fn=<MulBackward0>) 8260\n",
      "Loss: tensor(3.3430, grad_fn=<MulBackward0>) 8280\n",
      "Loss: tensor(3.4219, grad_fn=<MulBackward0>) 8300\n",
      "Loss: tensor(3.3652, grad_fn=<MulBackward0>) 8320\n",
      "Loss: tensor(3.3375, grad_fn=<MulBackward0>) 8340\n",
      "Loss: tensor(3.3426, grad_fn=<MulBackward0>) 8360\n",
      "Loss: tensor(3.4462, grad_fn=<MulBackward0>) 8380\n",
      "Loss: tensor(3.3431, grad_fn=<MulBackward0>) 8400\n",
      "Loss: tensor(3.2698, grad_fn=<MulBackward0>) 8420\n",
      "Loss: tensor(3.3689, grad_fn=<MulBackward0>) 8440\n",
      "Loss: tensor(3.3472, grad_fn=<MulBackward0>) 8460\n",
      "Loss: tensor(3.4161, grad_fn=<MulBackward0>) 8480\n",
      "Loss: tensor(3.2895, grad_fn=<MulBackward0>) 8500\n",
      "Loss: tensor(3.2239, grad_fn=<MulBackward0>) 8520\n",
      "Loss: tensor(3.4066, grad_fn=<MulBackward0>) 8540\n",
      "Loss: tensor(3.2896, grad_fn=<MulBackward0>) 8560\n",
      "Loss: tensor(3.3223, grad_fn=<MulBackward0>) 8580\n",
      "Loss: tensor(3.2855, grad_fn=<MulBackward0>) 8600\n",
      "Loss: tensor(3.3633, grad_fn=<MulBackward0>) 8620\n",
      "Loss: tensor(3.3715, grad_fn=<MulBackward0>) 8640\n",
      "Loss: tensor(3.4378, grad_fn=<MulBackward0>) 8660\n",
      "Loss: tensor(3.3211, grad_fn=<MulBackward0>) 8680\n",
      "Loss: tensor(3.3168, grad_fn=<MulBackward0>) 8700\n",
      "Loss: tensor(3.3561, grad_fn=<MulBackward0>) 8720\n",
      "Loss: tensor(3.3637, grad_fn=<MulBackward0>) 8740\n",
      "Loss: tensor(3.2543, grad_fn=<MulBackward0>) 8760\n",
      "Loss: tensor(3.2545, grad_fn=<MulBackward0>) 8780\n",
      "Loss: tensor(3.2965, grad_fn=<MulBackward0>) 8800\n",
      "Loss: tensor(3.3009, grad_fn=<MulBackward0>) 8820\n",
      "Loss: tensor(3.2372, grad_fn=<MulBackward0>) 8840\n",
      "Loss: tensor(3.3016, grad_fn=<MulBackward0>) 8860\n",
      "Loss: tensor(3.3583, grad_fn=<MulBackward0>) 8880\n",
      "Loss: tensor(3.3313, grad_fn=<MulBackward0>) 8900\n",
      "Loss: tensor(3.2406, grad_fn=<MulBackward0>) 8920\n",
      "Loss: tensor(3.3137, grad_fn=<MulBackward0>) 8940\n",
      "Loss: tensor(3.3518, grad_fn=<MulBackward0>) 8960\n",
      "Loss: tensor(3.3681, grad_fn=<MulBackward0>) 8980\n",
      "Loss: tensor(3.3263, grad_fn=<MulBackward0>) 9000\n",
      "Loss: tensor(3.3017, grad_fn=<MulBackward0>) 9020\n",
      "Loss: tensor(3.3202, grad_fn=<MulBackward0>) 9040\n",
      "Loss: tensor(3.3052, grad_fn=<MulBackward0>) 9060\n",
      "Loss: tensor(3.2511, grad_fn=<MulBackward0>) 9080\n",
      "Loss: tensor(3.3570, grad_fn=<MulBackward0>) 9100\n",
      "Loss: tensor(3.4005, grad_fn=<MulBackward0>) 9120\n",
      "Loss: tensor(3.2463, grad_fn=<MulBackward0>) 9140\n",
      "Loss: tensor(3.3796, grad_fn=<MulBackward0>) 9160\n",
      "Loss: tensor(3.3182, grad_fn=<MulBackward0>) 9180\n",
      "Loss: tensor(3.3503, grad_fn=<MulBackward0>) 9200\n",
      "Loss: tensor(3.3992, grad_fn=<MulBackward0>) 9220\n",
      "Loss: tensor(3.3369, grad_fn=<MulBackward0>) 9240\n",
      "Loss: tensor(3.3294, grad_fn=<MulBackward0>) 9260\n",
      "Loss: tensor(3.3826, grad_fn=<MulBackward0>) 9280\n",
      "Loss: tensor(3.2854, grad_fn=<MulBackward0>) 9300\n",
      "Loss: tensor(3.2914, grad_fn=<MulBackward0>) 9320\n",
      "Loss: tensor(3.3084, grad_fn=<MulBackward0>) 9340\n",
      "Loss: tensor(3.2023, grad_fn=<MulBackward0>) 9360\n",
      "Loss: tensor(3.2744, grad_fn=<MulBackward0>) 9380\n",
      "Loss: tensor(3.2950, grad_fn=<MulBackward0>) 9400\n",
      "Loss: tensor(3.2666, grad_fn=<MulBackward0>) 9420\n",
      "Loss: tensor(3.2710, grad_fn=<MulBackward0>) 9440\n",
      "Loss: tensor(3.3781, grad_fn=<MulBackward0>) 9460\n",
      "Loss: tensor(3.2888, grad_fn=<MulBackward0>) 9480\n",
      "Loss: tensor(3.2773, grad_fn=<MulBackward0>) 9500\n",
      "Loss: tensor(3.4038, grad_fn=<MulBackward0>) 9520\n",
      "Loss: tensor(3.2067, grad_fn=<MulBackward0>) 9540\n",
      "Loss: tensor(3.3397, grad_fn=<MulBackward0>) 9560\n",
      "Loss: tensor(3.3075, grad_fn=<MulBackward0>) 9580\n",
      "Loss: tensor(3.2800, grad_fn=<MulBackward0>) 9600\n",
      "Loss: tensor(3.3226, grad_fn=<MulBackward0>) 9620\n",
      "Loss: tensor(3.3049, grad_fn=<MulBackward0>) 9640\n",
      "Loss: tensor(3.2782, grad_fn=<MulBackward0>) 9660\n",
      "Loss: tensor(3.2123, grad_fn=<MulBackward0>) 9680\n",
      "Loss: tensor(3.2978, grad_fn=<MulBackward0>) 9700\n",
      "Loss: tensor(3.3136, grad_fn=<MulBackward0>) 9720\n",
      "Loss: tensor(3.3793, grad_fn=<MulBackward0>) 9740\n",
      "Loss: tensor(3.2938, grad_fn=<MulBackward0>) 9760\n",
      "Loss: tensor(3.3291, grad_fn=<MulBackward0>) 9780\n",
      "Loss: tensor(3.3120, grad_fn=<MulBackward0>) 9800\n",
      "Loss: tensor(3.3124, grad_fn=<MulBackward0>) 9820\n",
      "Loss: tensor(3.2834, grad_fn=<MulBackward0>) 9840\n",
      "Loss: tensor(3.3591, grad_fn=<MulBackward0>) 9860\n",
      "Loss: tensor(3.2386, grad_fn=<MulBackward0>) 9880\n",
      "Loss: tensor(3.2519, grad_fn=<MulBackward0>) 9900\n",
      "Loss: tensor(3.2445, grad_fn=<MulBackward0>) 9920\n",
      "Loss: tensor(3.3510, grad_fn=<MulBackward0>) 9940\n",
      "Loss: tensor(3.3090, grad_fn=<MulBackward0>) 9960\n",
      "Loss: tensor(3.3287, grad_fn=<MulBackward0>) 9980\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "samples = get_filename(\"./data/\")\n",
    "data = LoadData(file_path=\"./data/\", family=\"EGF\", samples=samples)\n",
    "\n",
    "adj_label_list = data.get_adj_label()\n",
    "adj_norm_list= data.get_adj_norm()\n",
    "adj_wgh_list = data.get_adj_wgh()\n",
    "adj_m = data.get_adj_m()\n",
    "x_list = data.get_feature()\n",
    "\n",
    "model = VGAE(38,1,1)\n",
    "optimizer = Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "for epoch in range(1000000):\n",
    "# init model and optimizer\n",
    "    loss_total = 0\n",
    "    for i in range(data.nSample):\n",
    "        adj_wgh = adj_wgh_list[i]\n",
    "        adj_norm = adj_norm_list[i]\n",
    "        adj_label = adj_label_list[i]\n",
    "        x_feature = x_list[i]\n",
    "        \n",
    "        A_pred = model(x_feature, adj_norm)\n",
    "        weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "        weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "        \n",
    "        pos_weight = float(adj_wgh.shape[0] * adj_wgh.shape[0] - adj_wgh.sum()) / adj_wgh.sum()\n",
    "        norm = adj_wgh.shape[0] * adj_wgh.shape[0] / float((adj_wgh.shape[0] * adj_wgh.shape[0] - adj_wgh.sum()) * 2)\n",
    "        weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "        loss =  norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "        \n",
    "        kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "        loss -= kl_divergence\n",
    "        loss_total += loss\n",
    "\n",
    "    loss_total = loss_total/data.nSample\n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Loss:\", 5*loss_total, epoch*20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b0d0757ee47d7ee28d024a252b5d7dd8b20484f5a8ae99b3e03790cf9a4c886"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
