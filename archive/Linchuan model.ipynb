{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30637842-522d-4ce1-a772-cc0d1d698458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "### predefined functions and classes ###\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "    init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "    return nn.Parameter(initial)\n",
    "\n",
    "def del_tensor_ele(arr, index):\n",
    "    arr1 = arr[0:index]\n",
    "    arr2 = arr[index+1:]\n",
    "    return torch.cat((arr1, arr2), dim=0)\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation = F.relu, **kwargs):\n",
    "        super(GraphConvSparse, self).__init__(**kwargs)\n",
    "        self.weight = glorot_init(input_dim, output_dim) \n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, adj, inputs):\n",
    "        sample = torch.randn(1, adj.size(1), self.output_dim)\n",
    "        for i in range(adj.size(0)):\n",
    "            x_ = inputs[i].reshape(inputs.size(1), inputs.size(2))\n",
    "            adj_ = adj[i].reshape(adj.size(1), adj.size(2))\n",
    "            x_ = torch.mm(x_,self.weight.double())\n",
    "            x_ = torch.mm(adj_, x_)\n",
    "            outputs = self.activation(x_)\n",
    "            outputs = outputs.reshape(1, adj.size(1), self.output_dim)\n",
    "            sample = torch.cat((sample, outputs), dim=0)\n",
    "        sample = del_tensor_ele(sample, 0)\n",
    "        return sample\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "    A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "    return A_pred\n",
    "\n",
    "### VGAE class ###\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.base_gcn = GraphConvSparse(input_dim, hidden1_dim)\n",
    "        self.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "        self.gcn_logstddev = GraphConvSparse(hidden1_dim, hidden2_dim, activation=lambda x:x)\n",
    "  \n",
    "    def encode(self, adj, X, n):\n",
    "        hidden = self.base_gcn(adj, X)\n",
    "        mean_ = self.gcn_mean(adj, hidden)\n",
    "        logstd = self.gcn_logstddev(adj, hidden)\n",
    "        var = torch.pow(torch.exp(logstd),2)\n",
    "        self.mean = torch.mean(mean_, axis=0).reshape(1, adj.size(1), hidden2_dim)\n",
    "        self.logstd = torch.log(torch.pow((torch.mean(var, axis=0)+torch.var(mean_, axis=0)),0.5)).reshape(1, adj.size(1), hidden2_dim)\n",
    "        sampled_z = torch.randn(1, adj.size(1), hidden2_dim)\n",
    "        for i in range(n):\n",
    "            gaussian_noise = torch.randn(adj.size(1), hidden2_dim).reshape(1, adj.size(1), hidden2_dim)\n",
    "            z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "            sampled_z = torch.cat((sampled_z, z), dim=0)\n",
    "        sampled_z = del_tensor_ele(sampled_z, 0)\n",
    "        return sampled_z\n",
    "    \n",
    "    def forward(self, adj, X, n):\n",
    "        Z = self.encode(adj, X, n)[0]\n",
    "        z = Z.reshape(adj.size(1), hidden2_dim)\n",
    "        A_pred = dot_product_decode(z).reshape(1, adj.size(1), adj.size(1))\n",
    "        A_pred_ = A_pred\n",
    "        for i in range(n-1):\n",
    "            A_pred = torch.cat((A_pred, A_pred_), dim=0)\n",
    "        return A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "303fe874-16a9-4d5d-8520-15c80dc2de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def norm_graph(adj):\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = np.diag(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt)\n",
    "    return adj_normalized\n",
    "\n",
    "def get_filename(mypath):\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    return f\n",
    "\n",
    "def compute_cent(mat):\n",
    "    return (np.sum(mat, axis=0))\n",
    "\n",
    "\n",
    "\n",
    "class LoadData():\n",
    "    \"\"\"Load graph data\"\"\"\n",
    "    def __init__(self, file_path, samples, non_orphan_list=None, n_node=38):\n",
    "        self.samples = samples\n",
    "        self.file_path = file_path\n",
    "        self.nSample = len(samples)\n",
    "        self.non_orphan_list = non_orphan_list\n",
    "        self.n_node = n_node\n",
    "        self.n_connect = len(non_orphan_list)\n",
    "\n",
    "    def get_adj_wgh(self):\n",
    "        \"\"\"generating weighted adjacency matrix\"\"\"\n",
    "        adj_orig_list =[]\n",
    "        for sample in self.samples:\n",
    "            f_name = self.file_path + \"/\"+ sample\n",
    "            adj = np.asarray(pd.read_csv(f_name, index_col = 0, iterator = False))\n",
    "            adj_orig_list.append(adj)    \n",
    "        return adj_orig_list\n",
    "\n",
    "    def get_adj_m(self):\n",
    "        \"\"\"generating the 0-1 adjacency matrix without diagonal elements\"\"\"\n",
    "        adj_wgh = self.get_adj_wgh()\n",
    "        adj_m_list =[]\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            adj[adj > 0] = 1\n",
    "            for i in range(self.n_node):\n",
    "                adj[i, i] = 0\n",
    "            for i in range(self.n_node):\n",
    "                for j in range(self.n_node):\n",
    "                    if adj[i, j] == 1:\n",
    "                        adj[j, i] = 1\n",
    "            for i in range(self.n_node - 1, -1, -1):\n",
    "                if i not in self.non_orphan_list:\n",
    "                    adj = np.delete(adj, (i), axis=0)\n",
    "                    adj = np.delete(adj, (i), axis=1)\n",
    "            adj_m_list.append(adj)    \n",
    "        return adj_m_list\n",
    "    \n",
    "    def adj_without(self):\n",
    "        adj_wgh = self.get_adj_wgh()\n",
    "        sample = np.eye(self.n_connect)\n",
    "        adj_without = np.array([sample])\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            adj[adj > 0] = 1\n",
    "            for i in range(self.n_node):\n",
    "                adj[i, i] = 0\n",
    "            for i in range(self.n_node):\n",
    "                for j in range(self.n_node):\n",
    "                    if adj[i, j] == 1:\n",
    "                        adj[j, i] = 1\n",
    "            for i in range(self.n_node - 1, -1, -1):\n",
    "                if i not in self.non_orphan_list:\n",
    "                    adj = np.delete(adj, (i), axis=0)\n",
    "                    adj = np.delete(adj, (i), axis=1)\n",
    "            adj = adj.astype(np.float)\n",
    "            adj_without = np.append(adj_without, [adj], axis=0)\n",
    "        adj_without = np.delete(adj_without, 0, axis=0)\n",
    "        return adj_without\n",
    "\n",
    "    def get_adj_m_t(self):\n",
    "        \"\"\"0-1 adjacency matrix without diagonal elements tensor format\"\"\"\n",
    "        adj_wgh = self.get_adj_wgh()\n",
    "        adj_m_list =[]\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            adj[adj > 0] = 1\n",
    "            for i in range(self.n_node):\n",
    "                adj[i, i] = 0\n",
    "            for i in range(self.n_node):\n",
    "                for j in range(self.n_node):\n",
    "                    if adj[i, j] == 1:\n",
    "                        adj[j, i] = 1\n",
    "            for i in range(self.n_node - 1, -1, -1):\n",
    "                if i not in self.non_orphan_list:\n",
    "                    adj = np.delete(adj, (i), axis=0)\n",
    "                    adj = np.delete(adj, (i), axis=1)\n",
    "            adj = sparse_to_tuple(sp.coo_matrix(adj))\n",
    "            adj = torch.sparse.FloatTensor(torch.LongTensor(adj[0].T), \n",
    "                            torch.FloatTensor(adj[1]), \n",
    "                            torch.Size(adj[2]))\n",
    "            adj_m_list.append(adj)    \n",
    "        return adj_m_list\n",
    "    \n",
    "    def get_adj_label(self):\n",
    "        \"\"\"0-1 adjancency matrix with diagonal elements tensor format\"\"\"\n",
    "        adj_m = self.get_adj_m()\n",
    "        adj_label_list =[]\n",
    "        \n",
    "        for _, adj in enumerate(adj_m):\n",
    "            adj_label = adj + sp.eye(adj.shape[0])\n",
    "            adj_label = sparse_to_tuple(sp.coo_matrix(adj_label))\n",
    "            #adj_label = sparse_to_tuple(adj_label)\n",
    "            adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "            adj_label_list.append(adj_label)\n",
    "        return adj_label_list\n",
    "\n",
    "    def adj_with(self):\n",
    "        adj_m = self.get_adj_m()\n",
    "        sample = np.eye(self.n_connect)\n",
    "        adj_with = np.array([sample])\n",
    "        for _, adj in enumerate(adj_m):\n",
    "            adj_label = adj + sp.eye(adj.shape[0])\n",
    "            adj_label = adj_label.astype(np.float)\n",
    "            adj_with = np.append(adj_with, [adj_label], axis=0)\n",
    "        adj_with = np.delete(adj_with, 0, axis=0)\n",
    "        return adj_with\n",
    "\n",
    "    def get_adj_norm(self):\n",
    "        \"\"\"0-1 normalized adjancency matrix tensor format\"\"\"\n",
    "        adj_m = self.get_adj_m()\n",
    "        adj_norm_list =[]\n",
    "\n",
    "        for _, adj in enumerate(adj_m):\n",
    "            adj_norm = preprocess_graph(adj)\n",
    "            adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "            adj_norm_list.append(adj_norm)\n",
    "        return adj_norm_list\n",
    "\n",
    "    def adj_norm(self):\n",
    "        adj_m = self.get_adj_m()\n",
    "        sample = np.eye(self.n_connect)\n",
    "        adj_norm = np.array([sample])\n",
    "        for _, adj in enumerate(adj_m):\n",
    "            adj_norm_ = norm_graph(adj)\n",
    "            adj_norm_ = adj_norm_.astype(np.float)\n",
    "            adj_norm = np.append(adj_norm, [adj_norm_], axis=0)\n",
    "        adj_norm = np.delete(adj_norm, 0, axis=0)\n",
    "        return adj_norm\n",
    "\n",
    "    def get_feature(self):\n",
    "        \"\"\"generating feature matrix X tensor format\"\"\"\n",
    "        adj_wgh = self.get_adj_wgh()\n",
    "        x_list = []\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            x_feature  = adj\n",
    "            x_feature  = csr_matrix(x_feature)\n",
    "            x_feature  = sparse_to_tuple(x_feature)\n",
    "            x_feature  = torch.sparse.FloatTensor(torch.LongTensor(x_feature[0].T), \n",
    "                            torch.FloatTensor(x_feature[1]), \n",
    "                            torch.Size(x_feature[2]))\n",
    "            x_list.append(x_feature)\n",
    "        return x_list\n",
    "\n",
    "    def get_x_feature(self):\n",
    "        \"\"\"one-hot encoding tensor format\"\"\"\n",
    "        x_onehot = torch.eye(len(self.non_orphan_list))\n",
    "        return torch.FloatTensor(x_onehot)\n",
    "\n",
    "    def one_hot(self):\n",
    "        sample = np.eye(self.n_connect)\n",
    "        feature = np.array([sample])\n",
    "        for i in range(self.nSample):\n",
    "            feature = np.append(feature,[np.eye(self.n_connect)],axis=0)\n",
    "        feature = np.delete(feature, 0, axis=0)\n",
    "        return feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f123ef3-c04b-45e5-9c42-6f574a9bfa3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 126>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mprint\u001b[39m(loss_avg)\n\u001b[0;32m--> 126\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    101\u001b[0m num_features \u001b[38;5;241m=\u001b[39m n_node\n\u001b[0;32m--> 103\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVGAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    105\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mVGAE.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_gcn \u001b[38;5;241m=\u001b[39m GraphConvSparse(input_dim, hidden1_dim)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_mean \u001b[38;5;241m=\u001b[39m GraphConvSparse(hidden1_dim, hidden2_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:x)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_logstddev \u001b[38;5;241m=\u001b[39m GraphConvSparse(hidden1_dim, hidden2_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:x)\n",
      "File \u001b[0;32m/gpfs/loomis/project/wang_zuoheng/jq87/conda_envs/ycrc_default/lib/python3.10/site-packages/torch/nn/modules/module.py:1214\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1216\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1217\u001b[0m     modules[name] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd.grad_mode import F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "class SimuData():\n",
    "    \"\"\"Simulate graph data\"\"\"\n",
    "    def __init__(self, n_node=10, n_graph=30):\n",
    "        self.n_node = n_node\n",
    "        self.n_graph = n_graph\n",
    "\n",
    "    def simu_adj_wgh(self):\n",
    "        adj_list = []\n",
    "\n",
    "        for i in range(self.n_graph):\n",
    "            ## zero matrix\n",
    "            A = torch.zeros(self.n_node, self.n_node)\n",
    "\n",
    "            # first five nodes weights: uniform(0,1)A[:5,:5] = W\n",
    "            W = torch.rand(5,5)\n",
    "\n",
    "            ## symmetric\n",
    "            i, j = torch.triu_indices(5, 5)\n",
    "            W[i, j] = W.T[i, j]\n",
    "\n",
    "            A[:5,:5] = W\n",
    "            adj_list.append(A)  \n",
    "\n",
    "        return adj_list\n",
    "\n",
    "    def simu_adj_diag(self):\n",
    "        adj_list = []\n",
    "\n",
    "        for i in range(self.n_graph):\n",
    "            A = torch.eye(self.n_node)\n",
    "            adj_list.append(A)  \n",
    "\n",
    "        return adj_list\n",
    "\n",
    "    def simu_adj_m(self):\n",
    "        \"\"\"generating adjacency matrix\"\"\"\n",
    "        adj_wgh = self.simu_adj_wgh()\n",
    "        #adj_wgh = self.simu_adj_diag()\n",
    "        adj_m_list =[]\n",
    "        for _, adj in enumerate(adj_wgh):\n",
    "            adj[adj>0.5] = 1\n",
    "            adj[adj<0.5] = 0\n",
    "            adj_m_list.append(adj)    \n",
    "        return adj_m_list\n",
    "\n",
    "    def graph_dataset(self):\n",
    "        dataset =[]\n",
    "        simu_adj = self.simu_adj_m()\n",
    "\n",
    "        for _, adj in enumerate(simu_adj):\n",
    "            edge_index_temp = sp.coo_matrix(adj)\n",
    "            indices = np.vstack((edge_index_temp.row, edge_index_temp.col))\n",
    "            edge_index_A = torch.LongTensor(indices) \n",
    "            x = self.get_x_feature()\n",
    "            data = Data(x=x, edge_index=edge_index_A)\n",
    "            dataset.append(data)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_x_feature(self):\n",
    "        x = torch.arange(self.n_node)\n",
    "        x_onehot = torch.eye(self.n_node)[x,:] \n",
    "\n",
    "        return torch.FloatTensor(x_onehot)\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) \n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "def train():\n",
    "    transform = T.Compose([\n",
    "                T.NormalizeFeatures(),\n",
    "                T.RandomLinkSplit(num_val=0, num_test=0, is_undirected=True,\n",
    "                                split_labels=True, add_negative_train_samples=False),])\n",
    "    \n",
    "    simu_graph = SimuData()\n",
    "    dataset = simu_graph.graph_dataset()\n",
    "    n_node = simu_graph.n_node\n",
    "    \n",
    "    out_channels = 2\n",
    "    num_features = n_node\n",
    "    \n",
    "    model = VGAE()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    epochs = 100\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_total = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i in range(len(dataset)):\n",
    "            train_data, val_data, test_data = transform(dataset[i])\n",
    "            z = model.encode(train_data.x, train_data.edge_index)\n",
    "            loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "            loss = loss + (1 /num_features) * model.kl_loss()\n",
    "            loss_total += loss\n",
    "\n",
    "        loss_avg = loss_total/len(dataset)\n",
    "        loss_avg.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss_avg)\n",
    "\n",
    "train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16164f84-4cf8-42f4-ad6e-dfb243ee7ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28338/3789869377.py:98: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  adj = adj.astype(np.float)\n",
      "/tmp/ipykernel_28338/3789869377.py:147: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  adj_label = adj_label.astype(np.float)\n",
      "/tmp/ipykernel_28338/3789869377.py:171: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  adj_norm_ = adj_norm_.astype(np.float)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.children() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     91\u001b[0m         param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m---> 93\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVGAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEncoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     96\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/gpfs/loomis/project/wang_zuoheng/jq87/conda_envs/ycrc_default/lib/python3.10/site-packages/torch_geometric/nn/models/autoencoder.py:147\u001b[0m, in \u001b[0;36mVGAE.__init__\u001b[0;34m(self, encoder, decoder)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder, decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/loomis/project/wang_zuoheng/jq87/conda_envs/ycrc_default/lib/python3.10/site-packages/torch_geometric/nn/models/autoencoder.py:64\u001b[0m, in \u001b[0;36mGAE.__init__\u001b[0;34m(self, encoder, decoder)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m encoder\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m InnerProductDecoder() \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m decoder\n\u001b[0;32m---> 64\u001b[0m \u001b[43mGAE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/loomis/project/wang_zuoheng/jq87/conda_envs/ycrc_default/lib/python3.10/site-packages/torch_geometric/nn/models/autoencoder.py:67\u001b[0m, in \u001b[0;36mGAE.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder)\n",
      "File \u001b[0;32m/gpfs/loomis/project/wang_zuoheng/jq87/conda_envs/ycrc_default/lib/python3.10/site-packages/torch_geometric/nn/inits.py:80\u001b[0m, in \u001b[0;36mreset\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     78\u001b[0m     value\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []:\n\u001b[1;32m     81\u001b[0m         reset(child)\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.children() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "from scipy.stats import ttest_ind, levene\n",
    "import scipy.stats as stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "### change working directory ###\n",
    "\n",
    "    \n",
    "### set parameters ###\n",
    "ipf_sample_size = 32\n",
    "control_sample_size = 28\n",
    "\n",
    "num_node = 38\n",
    "input_dim = 38\n",
    "hidden1_dim = 8\n",
    "hidden2_dim = 4\n",
    "num_epoch = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "w = 1e-4\n",
    "norm1 = 1\n",
    "norm2 = 1e-4\n",
    "\n",
    "###################################################################################\n",
    "######### data analysis for all_collagens_ipf_data ##########\n",
    "###################################################################################\n",
    "\n",
    "path_3 = './ST-Aim3/vgae_data/result_all_raw/EGF/IPF'  ## all_egf_ipf ##\n",
    "path_4 = './vgae_data/result_all_raw/EGF/Control'  ## all_egf_control ##\n",
    "path_9 = './vgae_data/result_sig_raw/EGF/IPF'  ## sig_egf_ipf ##\n",
    "path_10 = './vgae_data/result_sig_raw/EGF/Control'  ## sig_egf_control ##\n",
    "\n",
    "\n",
    "### specific parameters ###\n",
    "\n",
    "path = path_3\n",
    "sample_size = ipf_sample_size\n",
    "train_size = sample_size\n",
    "test_size = 0\n",
    "\n",
    "data_full = LoadData(samples=get_filename(path), file_path=path, non_orphan_list=range(38))\n",
    "cen_full = []\n",
    "cen = []\n",
    "\n",
    "for i in range(sample_size):\n",
    "    cen_full.append(compute_cent(data_full.get_adj_m()[i]))\n",
    "non_orphan = np.argwhere(np.sum(cen_full, axis=0)!=0).flatten().tolist()\n",
    "data = LoadData(file_path=path, samples=get_filename(path), non_orphan_list=non_orphan, n_node=num_node)\n",
    "for i in range(sample_size):\n",
    "    cen.append(compute_cent(data.get_adj_m()[i]))\n",
    "\n",
    "### construct data loader ###\n",
    "adj_without = torch.from_numpy(data.adj_without())\n",
    "adj_with = torch.from_numpy(data.adj_with())\n",
    "adj_norm = torch.from_numpy(data.adj_norm())\n",
    "one_hot = torch.from_numpy(data.one_hot())\n",
    "dataset = TensorDataset(adj_without, adj_with, adj_norm, one_hot)\n",
    "loader = DataLoader(dataset = dataset,batch_size = train_size)\n",
    "\n",
    "\n",
    "### train model for ipf data###\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate*(0.3**(epoch//100))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "model = VGAE(encoder=Encoder)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_value = []\n",
    "acc_value = []\n",
    "lik_value = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    t = time.time()\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    model.train()\n",
    "    for adj_without, adj_with, adj_norm, features in loader:\n",
    "\n",
    "        A_pred = model(adj_norm, features, sample_size)\n",
    "        mean = model.mean.reshape(adj_with.size(1), hidden2_dim)\n",
    "        logstd = model.logstd.reshape(adj_with.size(1), hidden2_dim)\n",
    "        loss = loss_function(w1,w2,w,norm1,norm2,A_pred,mean,logstd,adj_without,adj_with)\n",
    "        train_acc = get_acc(A_pred,adj_with)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_value.append(loss.item())\n",
    "        acc_value.append(train_acc.item())\n",
    "\n",
    "    if epoch % 5 ==0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()), \n",
    "              \"train_acc=\", \"{:.5f}\".format(train_acc),\"time=\", \"{:.5f}\".format(time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
